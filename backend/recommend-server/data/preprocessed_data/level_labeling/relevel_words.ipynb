{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:21.321046Z",
     "start_time": "2024-09-06T04:37:14.266019Z"
    }
   },
   "source": [
    "import csv\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 불용어 리스트\n",
    "stopwords = ['를', '을', '에', '가', '이', '은', '는', '하', '들', '사람', '너', '말', '좀', '걍', '잘', '걔', '한', '되', '음', '것', '과', '수', '다', '뒤', '나', '이다', '내', '그']\n",
    "\n",
    "# 추출할 품사 리스트 정의\n",
    "included_pos = ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "for i in range(1,7):\n",
    "    input_file = f\"C:/workspace/project_bigdata/data/textbook_data/kor-{i}.csv\"\n",
    "    output_file = f\"../data/raw/word/count/textbook/textbook-{i}.csv\"\n",
    "    \n",
    "    word_counter = Counter()\n",
    "    # count=0\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=',') \n",
    "    \n",
    "        for row in reader:\n",
    "            if row[1] == '빈도수':\n",
    "                continue\n",
    "            \n",
    "            if len(row) != 2:\n",
    "                print(f\"Unexpected row format: {row}\")\n",
    "                continue\n",
    "        \n",
    "            word, freq = row\n",
    "            freq = int(freq)\n",
    "            \n",
    "            # 형태소 분석 (stem=True 옵션으로 어근 추출)\n",
    "            morphs_with_pos = okt.pos(word, stem=True)\n",
    "            # print(f\"Original word: {word}, Morphs: {morphs_with_pos}\")  # 디버깅 용도\n",
    "            \n",
    "            # 어근 단어로 빈도수 합산\n",
    "            for morph, pos in morphs_with_pos:\n",
    "                if pos in included_pos and morph not in stopwords:\n",
    "                    word_counter[morph] += freq\n",
    "                # else:\n",
    "                #     count=count+1\n",
    "    \n",
    "    filtered_word_counter = {word: freq for word, freq in word_counter.items() if freq >= 30}\n",
    "    \n",
    "    row=0\n",
    "    # 결과를 새로운 CSV 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Word', 'Count'])\n",
    "        for i, (word, freq) in enumerate(filtered_word_counter.items(), start=1):\n",
    "            # print(f\"Original word: {word}, Freq: {freq}\")  # 디버깅 용도\n",
    "            writer.writerow([word, freq])\n",
    "            row=row+1\n",
    "    \n",
    "    print(f\"어근 단위로 통합된 단어 리스트가 '{output_file}' 파일에 저장되었습니다.\")\n",
    "    print(row)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-1.csv' 파일에 저장되었습니다.\n",
      "135\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-2.csv' 파일에 저장되었습니다.\n",
      "228\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-3.csv' 파일에 저장되었습니다.\n",
      "346\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-4.csv' 파일에 저장되었습니다.\n",
      "360\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-5.csv' 파일에 저장되었습니다.\n",
      "291\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/textbook/textbook-6.csv' 파일에 저장되었습니다.\n",
      "398\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:46.039313Z",
     "start_time": "2024-09-06T04:37:21.352381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# JSON 파일을 읽어서 DataFrame으로 변환하는 함수\n",
    "def json_files_to_word_count(folder_path):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "                try:\n",
    "                    json_data = json.load(f)\n",
    "                    # srcText 데이터 추출\n",
    "                    extracted_texts = extract_columns_from_json(json_data)\n",
    "                    all_texts.extend(extracted_texts)  # 모든 텍스트를 리스트에 추가\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {filename}: {e}\")\n",
    "    \n",
    "    # 전체 텍스트에 대해 형태소 분석 후 단어 빈도 계산\n",
    "    word_counter = analyze_texts_and_count_words(all_texts)\n",
    "    return word_counter\n",
    "\n",
    "# 특정 컬럼만 추출하기 위한 함수\n",
    "def extract_columns_from_json(json_data):\n",
    "    extracted_data = []\n",
    "    if 'paragraphInfo' in json_data:\n",
    "        for item in json_data['paragraphInfo']:\n",
    "            extracted_data.append(item.get(\"srcText\", \"\"))  # srcText만 추출\n",
    "    return extracted_data\n",
    "\n",
    "def analyze_texts_and_count_words(texts):\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        # 형태소 분석을 하고 품사 정보를 같이 반환\n",
    "        tokens_with_pos = okt.pos(text, norm=True, stem=True)\n",
    "        # 특정 품사에 해당하는 단어들만 필터링\n",
    "        filtered_tokens = [word for word, pos in tokens_with_pos if pos in included_pos and word not in stopwords]\n",
    "        word_counter.update(filtered_tokens)\n",
    "    return word_counter\n",
    "    \n",
    "# 빈도수가 100 이상인 단어들만 CSV 파일로 저장\n",
    "def save_frequent_words_to_csv(word_counter, output_file, min_count=100):\n",
    "    row=0           \n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Word', 'Count'])\n",
    "        for word, count in word_counter.items():\n",
    "            if count >= min_count:\n",
    "                print(f\"Original word: {word}, Freq: {count}\") \n",
    "                writer.writerow([word, count])\n",
    "                row=row+1\n",
    "    \n",
    "    print(f\"어근 단위로 통합된 단어 리스트가 '{output_file}' 파일에 저장되었습니다.\")\n",
    "    print(row)\n",
    "\n",
    "\n",
    "keyword = [\"preschool\", \"school_123\", \"school_456\"]\n",
    "\n",
    "for keyword in keyword:\n",
    "    # JSON 파일들이 있는 폴더 경로\n",
    "    folder_path = f\"C:/workspace/project_bigdata/data/story_test_data/{keyword}\"\n",
    "    output_file = f\"../data/raw/word/count/story/story_{keyword}.csv\"\n",
    "    # 전체 작업 실행\n",
    "    word_counter = json_files_to_word_count(folder_path)\n",
    "    save_frequent_words_to_csv(word_counter, output_file)\n",
    "\n"
   ],
   "id": "771eb7cb84856899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: 하다, Freq: 757\n",
      "Original word: 있다, Freq: 462\n",
      "Original word: 가다, Freq: 104\n",
      "Original word: 되다, Freq: 108\n",
      "Original word: 엄마, Freq: 143\n",
      "Original word: 보다, Freq: 162\n",
      "Original word: 먹다, Freq: 103\n",
      "Original word: 만들다, Freq: 103\n",
      "Original word: 우리, Freq: 103\n",
      "Original word: 그림, Freq: 158\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/story/story_preschool.csv' 파일에 저장되었습니다.\n",
      "10\n",
      "Original word: 아빠, Freq: 124\n",
      "Original word: 가다, Freq: 128\n",
      "Original word: 하다, Freq: 975\n",
      "Original word: 없다, Freq: 137\n",
      "Original word: 보다, Freq: 272\n",
      "Original word: 우리, Freq: 147\n",
      "Original word: 주다, Freq: 143\n",
      "Original word: 있다, Freq: 529\n",
      "Original word: 않다, Freq: 152\n",
      "Original word: 오다, Freq: 172\n",
      "Original word: 모두, Freq: 102\n",
      "Original word: 친구, Freq: 107\n",
      "Original word: 엄마, Freq: 162\n",
      "Original word: 안, Freq: 142\n",
      "Original word: 좋다, Freq: 125\n",
      "Original word: 그렇다, Freq: 161\n",
      "Original word: 나다, Freq: 114\n",
      "Original word: 되다, Freq: 126\n",
      "Original word: 먹다, Freq: 155\n",
      "Original word: 마리, Freq: 102\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/story/story_school_123.csv' 파일에 저장되었습니다.\n",
      "20\n",
      "Original word: 고슴도치, Freq: 166\n",
      "Original word: 라테, Freq: 327\n",
      "Original word: 하다, Freq: 2163\n",
      "Original word: 있다, Freq: 1220\n",
      "Original word: 같다, Freq: 168\n",
      "Original word: 때, Freq: 216\n",
      "Original word: 나무, Freq: 129\n",
      "Original word: 소리, Freq: 160\n",
      "Original word: 다람쥐, Freq: 108\n",
      "Original word: 그것, Freq: 102\n",
      "Original word: 물, Freq: 106\n",
      "Original word: 나오다, Freq: 105\n",
      "Original word: 숲, Freq: 169\n",
      "Original word: 늑대, Freq: 267\n",
      "Original word: 크다, Freq: 167\n",
      "Original word: 앞, Freq: 101\n",
      "Original word: 츄움, Freq: 119\n",
      "Original word: 없다, Freq: 342\n",
      "Original word: 일, Freq: 121\n",
      "Original word: 그렇다, Freq: 197\n",
      "Original word: 때문, Freq: 150\n",
      "Original word: 위, Freq: 113\n",
      "Original word: 보다, Freq: 238\n",
      "Original word: 곳, Freq: 195\n",
      "Original word: 아니다, Freq: 102\n",
      "Original word: 저, Freq: 103\n",
      "Original word: 생각, Freq: 187\n",
      "Original word: 오다, Freq: 171\n",
      "Original word: 속, Freq: 174\n",
      "Original word: 들다, Freq: 189\n",
      "Original word: 않다, Freq: 323\n",
      "Original word: 가다, Freq: 226\n",
      "Original word: 바위, Freq: 117\n",
      "Original word: 동물, Freq: 100\n",
      "Original word: 곰, Freq: 275\n",
      "Original word: 우리, Freq: 151\n",
      "Original word: 시작, Freq: 123\n",
      "Original word: 먹다, Freq: 105\n",
      "Original word: 자기, Freq: 146\n",
      "Original word: 만들다, Freq: 122\n",
      "Original word: 어떻다, Freq: 120\n",
      "Original word: 많다, Freq: 101\n",
      "Original word: 주다, Freq: 128\n",
      "Original word: 되어다, Freq: 223\n",
      "Original word: 되다, Freq: 271\n",
      "Original word: 날, Freq: 107\n",
      "Original word: 돈, Freq: 157\n",
      "Original word: 새끼, Freq: 109\n",
      "Original word: 워브, Freq: 316\n",
      "Original word: 여우, Freq: 107\n",
      "Original word: 알렉산더, Freq: 150\n",
      "Original word: 로마, Freq: 128\n",
      "Original word: 카이사르, Freq: 128\n",
      "Original word: 한니발, Freq: 125\n",
      "어근 단위로 통합된 단어 리스트가 '../data/raw/word/count/story/story_school_456.csv' 파일에 저장되었습니다.\n",
      "54\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:46.087902Z",
     "start_time": "2024-09-06T04:37:46.039313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(filepath):\n",
    "    return pd.read_csv(filepath, sep=',')\n",
    "\n",
    "def calculate_points(textbook_file, story_file):\n",
    "    textbook_df = read_file(textbook_file)\n",
    "    story_df = read_file(story_file)\n",
    "\n",
    "    # 단어 기준으로 병합 (outer join으로 공통 단어 + 한쪽만 있는 단어 모두 포함)\n",
    "    merged_df = pd.merge(textbook_df, story_df, on='Word', how='outer', suffixes=('_textbook', '_story'))\n",
    "    \n",
    "    # NaN을 0으로 변환하고, 빈도수 합산\n",
    "    merged_df.fillna(0, inplace=True)\n",
    "    merged_df['Point'] = merged_df['Count_textbook']*3 + merged_df['Count_story']\n",
    "    merged_df.drop(['Count_textbook', 'Count_story'], axis=1, inplace=True)\n",
    "    print(merged_df)\n",
    "    return merged_df\n",
    "\n",
    "def convert_point(story_file):\n",
    "    story_df = read_file(story_file)\n",
    "    story_df['Point'] = story_df['Count']\n",
    "    story_df.drop(['Count'], axis=1, inplace=True)\n",
    "    print(story_df)\n",
    "    return story_df\n",
    "\n",
    "# 키워드와 파일 설정\n",
    "textbook_files = [f\"../data/raw/word/count/textbook/textbook-{i}.csv\" for i in range(1, 7)]\n",
    "story_files = {\n",
    "    \"school_123\": \"../data/raw/word/count/story/story_school_123.csv\",\n",
    "    \"school_456\": \"../data/raw/word/count/story/story_school_456.csv\"\n",
    "}\n",
    "\n",
    "merged_df = convert_point(\"../data/raw/word/count/story/story_preschool.csv\")\n",
    "output_file = f\"../data/raw/word/count/combined_{1}.csv\"\n",
    "print(output_file)\n",
    "merged_df.to_csv(output_file, sep=',', index=False, encoding='utf-8-sig')\n",
    "for i in range(1, 7):\n",
    "    textbook_file = f\"../data/raw/word/count/textbook/textbook-{i}.csv\"\n",
    "    \n",
    "    if i <= 3:\n",
    "        story_file = story_files[\"school_123\"]\n",
    "    else:\n",
    "        story_file = story_files[\"school_456\"]\n",
    "    \n",
    "    # textbook과 story 파일 병합\n",
    "    merged_df = calculate_points(textbook_file, story_file)\n",
    "    output_file = f\"../data/raw/word/count/combined_{i+1}.csv\"\n",
    "    print(output_file)\n",
    "    merged_df.to_csv(output_file, sep=',', index=False, encoding='utf-8-sig')\n"
   ],
   "id": "f85e7522f2cf30d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word  Point\n",
      "0   하다    757\n",
      "1   있다    462\n",
      "2   가다    104\n",
      "3   되다    108\n",
      "4   엄마    143\n",
      "5   보다    162\n",
      "6   먹다    103\n",
      "7  만들다    103\n",
      "8   우리    103\n",
      "9   그림    158\n",
      "../data/raw/word/count/combined_1.csv\n",
      "    Word  Point\n",
      "0     가다  470.0\n",
      "1     가지  189.0\n",
      "2     겪다  168.0\n",
      "3    고르다   93.0\n",
      "4    고맙다  105.0\n",
      "..   ...    ...\n",
      "133   행동  123.0\n",
      "134   확인  249.0\n",
      "135   활동  132.0\n",
      "136   활용  303.0\n",
      "137   흉내  120.0\n",
      "\n",
      "[138 rows x 2 columns]\n",
      "../data/raw/word/count/combined_2.csv\n",
      "    Word  Point\n",
      "0     가게   96.0\n",
      "1     가다  692.0\n",
      "2    가운데  141.0\n",
      "3     가족  201.0\n",
      "4     가지  201.0\n",
      "..   ...    ...\n",
      "224    형  120.0\n",
      "225  호랑이  126.0\n",
      "226   확인  270.0\n",
      "227   활동  168.0\n",
      "228   활용  291.0\n",
      "\n",
      "[229 rows x 2 columns]\n",
      "../data/raw/word/count/combined_3.csv\n",
      "    Word   Point\n",
      "0     가다  1073.0\n",
      "1    가운데   213.0\n",
      "2     가장   198.0\n",
      "3     가지   276.0\n",
      "4      간   252.0\n",
      "..   ...     ...\n",
      "343  호랑이   141.0\n",
      "344   확인   219.0\n",
      "345   활동   207.0\n",
      "346   활용   252.0\n",
      "347    후   114.0\n",
      "\n",
      "[348 rows x 2 columns]\n",
      "../data/raw/word/count/combined_4.csv\n",
      "    Word  Point\n",
      "0     가다  937.0\n",
      "1    가운데  186.0\n",
      "2     가장  126.0\n",
      "3     가족   93.0\n",
      "4     가지  327.0\n",
      "..   ...    ...\n",
      "370   활동  216.0\n",
      "371   활용  213.0\n",
      "372   회의  459.0\n",
      "373    후   93.0\n",
      "374   흐름  111.0\n",
      "\n",
      "[375 rows x 2 columns]\n",
      "../data/raw/word/count/combined_5.csv\n",
      "    Word  Point\n",
      "0     가다  940.0\n",
      "1    가운데  237.0\n",
      "2     가장  168.0\n",
      "3     가지  315.0\n",
      "4     감상  162.0\n",
      "..   ...    ...\n",
      "304   확인  201.0\n",
      "305   활동  123.0\n",
      "306   활용  564.0\n",
      "307    흑  105.0\n",
      "308    힘   90.0\n",
      "\n",
      "[309 rows x 2 columns]\n",
      "../data/raw/word/count/combined_6.csv\n",
      "    Word   Point\n",
      "0     가다  1102.0\n",
      "1    가운데   135.0\n",
      "2     가장   123.0\n",
      "3     가지   195.0\n",
      "4     가치   267.0\n",
      "..   ...     ...\n",
      "409   활동   177.0\n",
      "410   활용   861.0\n",
      "411   효과   156.0\n",
      "412    후   102.0\n",
      "413  힘들다    99.0\n",
      "\n",
      "[414 rows x 2 columns]\n",
      "../data/raw/word/count/combined_7.csv\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:46.157285Z",
     "start_time": "2024-09-06T04:37:46.111189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def combine_results(file_pattern, num_files):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = file_pattern.format(i)\n",
    "        df = pd.read_csv(file_path, sep=',')\n",
    "        df['Grade'] = str(i)+\"레벨\"  # Add grade information\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def prepare_dtm(combined_df):\n",
    "    dtm_df = combined_df.pivot_table(index='Word', columns='Grade', values='Point', fill_value=0)\n",
    "    dtm_df = dtm_df.reset_index()\n",
    "    return dtm_df\n",
    "\n",
    "def calculate_tfidf(dtm_df):\n",
    "    dtm_df = dtm_df.reset_index()  # Reset index to ensure 'Word' is a column\n",
    "    dtm_df.set_index('Word', inplace=True) \n",
    "    # Convert the DataFrame to a format suitable for TF-IDF calculation\n",
    "    dtm_text = dtm_df.apply(lambda x: ' '.join([f\"{col}\" for col in x.index if x[col] > 0]), axis=1)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(dtm_text)\n",
    "    \n",
    "    # Create DataFrame for TF-IDF scores\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=dtm_df.index, columns=vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "\n",
    "file_pattern = \"../data/raw/word/count/combined_{}.csv\"\n",
    "textbook_df = combine_results(file_pattern, 7)\n",
    "\n",
    "# Prepare document-term matrix\n",
    "dtm_df = prepare_dtm(textbook_df)\n",
    "dtm_df.to_csv(\"../data/raw/word/count/combined.csv\", sep=',', encoding='utf-8-sig')\n",
    "print(dtm_df)\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tfidf_df = calculate_tfidf(dtm_df)\n",
    "\n",
    "# Save the TF-IDF results\n",
    "tfidf_df.to_csv(\"../data/raw/word/count/combined_tfidf.csv\", sep=',', encoding='utf-8-sig')\n",
    "print(\"TF-IDF analysis completed and saved.\")\n"
   ],
   "id": "6ae605e67ec12240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Word    1레벨    2레벨    3레벨     4레벨    5레벨    6레벨     7레벨\n",
      "0       가게    0.0    0.0   96.0     0.0    0.0    0.0     0.0\n",
      "1       가다  104.0  470.0  692.0  1073.0  937.0  940.0  1102.0\n",
      "2      가운데    0.0    0.0  141.0   213.0  186.0  237.0   135.0\n",
      "3       가장    0.0    0.0    0.0   198.0  126.0  168.0   123.0\n",
      "4       가족    0.0    0.0  201.0     0.0   93.0    0.0     0.0\n",
      "..     ...    ...    ...    ...     ...    ...    ...     ...\n",
      "677     흉내    0.0  120.0    0.0     0.0    0.0    0.0     0.0\n",
      "678     흐름    0.0    0.0    0.0     0.0  111.0    0.0     0.0\n",
      "679      흑    0.0    0.0    0.0     0.0    0.0  105.0     0.0\n",
      "680      힘    0.0    0.0    0.0     0.0    0.0   90.0     0.0\n",
      "681    힘들다    0.0    0.0    0.0     0.0    0.0    0.0    99.0\n",
      "\n",
      "[682 rows x 8 columns]\n",
      "TF-IDF analysis completed and saved.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:46.184088Z",
     "start_time": "2024-09-06T04:37:46.180302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# def generate_word_cloud(tfidf_df, grade):\n",
    "#     # 특정 학년에 대한 TF-IDF 점수 필터링\n",
    "#     if grade not in tfidf_df.columns:\n",
    "#         print(f\"데이터프레임에 '{grade}' 학년이 없습니다.\")\n",
    "#         return\n",
    "#     \n",
    "#     grade_tfidf = tfidf_df[grade]\n",
    "#     \n",
    "#     # TF-IDF 점수를 딕셔너리로 변환\n",
    "#     word_freq = grade_tfidf.to_dict()\n",
    "#     font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "#     \n",
    "#     # 워드 클라우드 생성\n",
    "#     wordcloud = WordCloud(font_path=font_path,width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "#     \n",
    "#     # 워드 클라우드 표시\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"{grade}의 워드 클라우드\")\n",
    "#     plt.show()\n",
    "# \n",
    "# print(\"TF-IDF DataFrame 열 이름:\", tfidf_df.columns)\n",
    "# \n",
    "# # 각 학년별 워드 클라우드 생성 및 표시\n",
    "# for grade in range(1, 7):\n",
    "#     generate_word_cloud(tfidf_df, str(grade)+\"레벨\")\n"
   ],
   "id": "1e10b8ccf99d79a3",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:37:46.226575Z",
     "start_time": "2024-09-06T04:37:46.206789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_tfidf(tfidf_df, threshold=0.5):\n",
    "    results = {}\n",
    "    excluded_words = set()\n",
    "    \n",
    "    for grade in tfidf_df.columns:\n",
    "        if grade == 'index':\n",
    "            continue\n",
    "        \n",
    "        high_tfidf_words = tfidf_df[tfidf_df[grade] > threshold][[grade]]\n",
    "        \n",
    "        high_tfidf_words = high_tfidf_words[~high_tfidf_words.index.isin(excluded_words)]\n",
    "        \n",
    "        excluded_words.update(high_tfidf_words.index)\n",
    "        \n",
    "        # Store the results\n",
    "        results[grade] = high_tfidf_words.reset_index()  # Remove 'Word' index\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_filtered_results(filtered_results):\n",
    "    for grade, df in filtered_results.items():\n",
    "        file_path = f\"../data/raw/word/filtered_{grade}.csv\"\n",
    "        df.to_csv(file_path, index=False, sep=',', encoding='utf-8-sig')\n",
    "        print(f\"Saved filtered results for {grade} to {file_path}\")\n",
    "\n",
    "tfidf_df = pd.read_csv(\"../data/raw/word/count/combined_tfidf.csv\", sep=',', index_col=0)\n",
    "filtered_results = filter_tfidf(tfidf_df)\n",
    "save_filtered_results(filtered_results)"
   ],
   "id": "1629e39223c5af45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered results for 1레벨 to ../data/raw/word/filtered_1레벨.csv\n",
      "Saved filtered results for 2레벨 to ../data/raw/word/filtered_2레벨.csv\n",
      "Saved filtered results for 3레벨 to ../data/raw/word/filtered_3레벨.csv\n",
      "Saved filtered results for 4레벨 to ../data/raw/word/filtered_4레벨.csv\n",
      "Saved filtered results for 5레벨 to ../data/raw/word/filtered_5레벨.csv\n",
      "Saved filtered results for 6레벨 to ../data/raw/word/filtered_6레벨.csv\n",
      "Saved filtered results for 7레벨 to ../data/raw/word/filtered_7레벨.csv\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T04:49:43.321473Z",
     "start_time": "2024-09-06T04:49:43.276556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combined_df = pd.DataFrame(columns=['Word', 'Level'])\n",
    "\n",
    "for i in range(1,8):\n",
    "    filtered = pd.read_csv(f\"../data/raw/word/filtered_{i}레벨.csv\", sep=',', index_col=0)\n",
    "    kice = pd.read_csv(f\"../data/raw/word/kice_{i}레벨.csv\", encoding='cp949', sep=',')\n",
    "    \n",
    "    filtered['Level'] = i\n",
    "    filtered['Word'] = filtered.index\n",
    "    kice['Level'] = i\n",
    "    \n",
    "    combined = pd.concat([filtered, kice], ignore_index=False)\n",
    "    combined.reset_index(inplace=True, drop=True)\n",
    "    combined = combined[['Word', 'Level']]\n",
    "    # combined_df에 중복되지 않은 단어만 추가\n",
    "    combined = combined[~combined['Word'].isin(combined_df['Word'])]\n",
    "    \n",
    "    combined.sort_values(by='Word').reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    combined_df = pd.concat([combined_df, combined], ignore_index=True)\n",
    "    print(combined_df)\n",
    "    \n",
    "\n",
    "file_path = f\"../data/processed/word_dictionary.csv\"\n",
    "combined_df.to_csv(file_path, index=False, sep=',', encoding='utf-8-sig')\n",
    "print(f\"Saved dictionary to {file_path}\")\n",
    "\n",
    "# 해야할일\n",
    "# 말도 안되는 단어 거르기 - 사전 API 활용... 귀찮은데...\n"
   ],
   "id": "967d5fbfdaff0bf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Word Level\n",
      "0     가다     1\n",
      "1     그림     1\n",
      "2     되다     1\n",
      "3    만들다     1\n",
      "4     먹다     1\n",
      "..   ...   ...\n",
      "518  화요일     1\n",
      "519  화장실     1\n",
      "520  흔들다     1\n",
      "521    흙     1\n",
      "522   흰색     1\n",
      "\n",
      "[523 rows x 2 columns]\n",
      "       Word Level\n",
      "0        가다     1\n",
      "1        그림     1\n",
      "2        되다     1\n",
      "3       만들다     1\n",
      "4        먹다     1\n",
      "...     ...   ...\n",
      "1021     훨훨     2\n",
      "1022  휴대 전화     2\n",
      "1023     흉내     2\n",
      "1024      힘     2\n",
      "1025    힘들다     2\n",
      "\n",
      "[1026 rows x 2 columns]\n",
      "      Word Level\n",
      "0       가다     1\n",
      "1       그림     1\n",
      "2       되다     1\n",
      "3      만들다     1\n",
      "4       먹다     1\n",
      "...    ...   ...\n",
      "1390    훨씬     3\n",
      "1391  흩어지다     3\n",
      "1392    희망     3\n",
      "1393   힘쓰다     3\n",
      "1394   힘없이     3\n",
      "\n",
      "[1395 rows x 2 columns]\n",
      "      Word Level\n",
      "0       가다     1\n",
      "1       그림     1\n",
      "2       되다     1\n",
      "3      만들다     1\n",
      "4       먹다     1\n",
      "...    ...   ...\n",
      "1675  흐뭇하다     4\n",
      "1676   흥겹다     4\n",
      "1677    흥미     4\n",
      "1678  흥미롭다     4\n",
      "1679   힘차다     4\n",
      "\n",
      "[1680 rows x 2 columns]\n",
      "      Word Level\n",
      "0       가다     1\n",
      "1       그림     1\n",
      "2       되다     1\n",
      "3      만들다     1\n",
      "4       먹다     1\n",
      "...    ...   ...\n",
      "1824  풍부하다     5\n",
      "1825   해로움     5\n",
      "1826  활용하다     5\n",
      "1827    훗날     5\n",
      "1828    흐름     5\n",
      "\n",
      "[1829 rows x 2 columns]\n",
      "      Word Level\n",
      "0       가다     1\n",
      "1       그림     1\n",
      "2       되다     1\n",
      "3      만들다     1\n",
      "4       먹다     1\n",
      "...    ...   ...\n",
      "1885  찬란하다     6\n",
      "1886    탄력     6\n",
      "1887   턱없이     6\n",
      "1888    학비     6\n",
      "1889   한없이     6\n",
      "\n",
      "[1890 rows x 2 columns]\n",
      "     Word Level\n",
      "0      가다     1\n",
      "1      그림     1\n",
      "2      되다     1\n",
      "3     만들다     1\n",
      "4      먹다     1\n",
      "...   ...   ...\n",
      "1949   하자     7\n",
      "1950   허련     7\n",
      "1951   호머     7\n",
      "1952    홍     7\n",
      "1953   효과     7\n",
      "\n",
      "[1954 rows x 2 columns]\n",
      "Saved dictionary to ../data/raw/word/word_dictionary.csv\n"
     ]
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
