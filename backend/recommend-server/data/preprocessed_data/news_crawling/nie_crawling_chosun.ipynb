{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T04:48:26.285678Z",
     "start_time": "2024-09-10T04:47:56.530464Z"
    }
   },
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    " \n",
    "path = \"C:\\\\workspace\\\\특화프로젝트\\\\chromedriver-win64\\\\chromedriver.exe\" # 웹드라이버 실행\n",
    " \n",
    "driver = webdriver.Chrome() # 드라이버 경로 설정\n",
    "url_list = [] # url을 저장하기 위한 변수\n",
    "content_list = []\n",
    "\n",
    "\n",
    "# 페이지를 순회합니다.\n",
    "for i in range(1, 11):\n",
    "    url = f'https://bbs.chosun.com/messagelist/list.bbs?bbs_id=9999300015&current_page={i}'\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # 페이지 로딩을 위해 잠시 대기합니다\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # 게시글 목록 추출\n",
    "    rows = soup.find_all('tr', {'height': '28'})\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            title_url = \"https://bbs.chosun.com\"+row.find('td', {'class': 'list_txt02'}).find('a')['href']\n",
    "            url_list.append(title_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding element: {e}\")\n",
    "\n",
    "print(\"url 수집 끝\")\n",
    "driver.close()\n",
    "\n",
    "print(\"수집한 url 개수 : \"+str(len(url_list)))\n",
    "# print(url_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 수집 끝\n",
      "수집한 url 개수 : 200\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T04:57:55.346616Z",
     "start_time": "2024-09-10T04:48:26.292686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "df = pd.DataFrame(columns=['URL', 'title', 'content', 'question']) # content를 누적하기 위한 변수\n",
    "\n",
    "for index, url in enumerate(url_list):\n",
    "    title_text = \"\"\n",
    "    content_text = \"\"\n",
    "    question_text = \"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    row = soup.find('tr', {'height': '29px', 'class': 'ivory_titleBg'})\n",
    "    title_tag = row.find('td', {'colspan': '7', 'class': 'list_txt05'}).find('b')\n",
    "    if title_tag:\n",
    "        title_text = title_tag.text.strip()\n",
    "    else:\n",
    "        title_text = \"No Title Found\"\n",
    "        \n",
    "    # 내용 추출\n",
    "    content_div = soup.find('div', {'id': 'blogContents'})\n",
    "    if content_div:\n",
    "        spans = content_div.find_all('span', style=lambda value: value and 'font-size: 14pt' in value and 'line-height:1.8' in value)\n",
    "    \n",
    "        # span 태그의 텍스트를 추출합니다.\n",
    "        content_text = \"\\n\".join(span.get_text(separator='\\n', strip=True) for span in spans)\n",
    "    \n",
    "    # 질문 추출\n",
    "    question_divs = soup.find_all('p', style=lambda value: value and 'background:#f5efe5;' in value)\n",
    "    if question_divs:\n",
    "        questions = [q.get_text(strip=True) for q in question_divs]\n",
    "        question_text = '\\n'.join(questions)\n",
    "        \n",
    "    content_list.append({\n",
    "        'URL': url,\n",
    "        'title': title_text,\n",
    "        'content': content_text,\n",
    "        'question': question_text\n",
    "    })\n",
    "    \n",
    "    if (index + 1) % 10 == 0:\n",
    "        print(f\"Processed {index + 1} URLs\")\n",
    "\n",
    "df = pd.DataFrame(content_list)\n",
    "# print(df)\n",
    "\n",
    "driver.quit()"
   ],
   "id": "220600d40b6e178d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 URLs\n",
      "Processed 20 URLs\n",
      "Processed 30 URLs\n",
      "Processed 40 URLs\n",
      "Processed 50 URLs\n",
      "Processed 60 URLs\n",
      "Processed 70 URLs\n",
      "Processed 80 URLs\n",
      "Processed 90 URLs\n",
      "Processed 100 URLs\n",
      "Processed 110 URLs\n",
      "Processed 120 URLs\n",
      "Processed 130 URLs\n",
      "Processed 140 URLs\n",
      "Processed 150 URLs\n",
      "Processed 160 URLs\n",
      "Processed 170 URLs\n",
      "Processed 180 URLs\n",
      "Processed 190 URLs\n",
      "Processed 200 URLs\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T04:57:55.484663Z",
     "start_time": "2024-09-10T04:57:55.473143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../data/raw/news/nie_chosun_crawled.csv\"\n",
    "df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data saved\")"
   ],
   "id": "fe7a873a89f0abd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
